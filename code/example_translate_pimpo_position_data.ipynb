{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine-translate quasi-sentences in position-coded  subset of the PimPo dataset\n",
    "\n",
    "| Authors | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2023-12-07 |\n",
    "\n",
    "<br>\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/fabiennelind/Going-Cross-Lingual_Course/blob/main/code/example_translate_pimpo_position_data.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "We use the [PimPo](https://manifesto-project.wzb.eu/information/documents/pimpo) dataset that records party manifesto quasi-sentences coded for positions on immigration or integration.\n",
    "We have already subset this data to quasi-sentences mentioning the issues of immigration or integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except:\n",
    "    COLAB=False\n",
    "print('on colab:', COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install libraries if on Colab\n",
    "%%capture\n",
    "if COLAB:\n",
    "    !pip install iso639==0.1.4 easynmt==2.0.0 deepl==1.16.1 google-cloud-translate==3.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import iso639\n",
    "\n",
    "import torch\n",
    "\n",
    "import easynmt\n",
    "import deepl\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import translate_v2 as gt\n",
    "\n",
    "base_path = os.path.join('..')\n",
    "data_path = os.path.join(base_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = os.path.join(data_path, 'lehmann+zobel_2018_pimpo_positions_translated.tsv')\n",
    "fp = 'https://raw.githubusercontent.com/fabiennelind/Going-Cross-Lingual_Course/main/data/lehmann%2Bzobel_2018_pimpo_positions.tsv'\n",
    "df = pd.read_csv(fp, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.position.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# characters =~ {df.text.apply(len).sum() / 1_000_000:.03f} mio')\n",
    "print(f'approximate costs =~ {(df.text.apply(len).sum() / 1_000_000)*20:.02f} EUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definining the translation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk list of sentences into smaller chunks\n",
    "def chunk(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clean_memory(device):\n",
    "    if 'cuda' in str(device):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    elif 'mps' in str(device):\n",
    "        torch.mps.empty_cache()\n",
    "    else:\n",
    "        pass\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable, Union\n",
    "\n",
    "def translate_batch_safely(texts: list, translation_fun: Callable, device: Union[str, torch.device], **kwargs) -> list:\n",
    "    \"\"\"\n",
    "    Translates a batch of texts using the model, handling potential errors.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list): A list of texts to be translated.\n",
    "        translation_fun (Callable): The translation function to be used.\n",
    "        **kwargs: Additional keyword arguments to be passed to `translation_fun`.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of translated texts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to translate the batch of texts using the model\n",
    "        res = translation_fun(texts, **kwargs)\n",
    "    except Exception as e:\n",
    "        # If the exception is _not_ related to running out of memory, ...\n",
    "        if 'out of memory' not in str(e):\n",
    "            # ... raise the exception\n",
    "            raise e\n",
    "        # but if the error was due running out of memory, ...\n",
    "        else:\n",
    "            clean_memory(device)\n",
    "            res = [None] * len(texts)\n",
    "            # ... try translating each text individually\n",
    "            for i, text in enumerate(texts):\n",
    "                try:\n",
    "                    res[i] = translation_fun(text, batch_size=1, **kwargs)\n",
    "                except:\n",
    "                    # If unable to translate a text, print a warning message\n",
    "                    print(f'WARNING: couldn\\'t translate text \"{text}\")')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def translate_in_batches(texts: list, batch_size, verbose=False, pbar_desc=None, **kwargs) -> list:\n",
    "    \"\"\"\n",
    "    Translates a list of texts in batches using the `translate_batch_safely` function.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list): A list of texts to be translated.\n",
    "        batch_size (int): The size of each translation batch.\n",
    "        verbose (bool): Whether to print messages and a progress bar\n",
    "        pbard_desc (str): The description of the progress bar\n",
    "        **kwargs: Additional keyword arguments to be passed to the `translate_batch_safely` function.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of translated texts.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the translations\n",
    "    translations = []\n",
    "    n_batches = len(texts)//batch_size\n",
    "    if verbose:\n",
    "        pbar = tqdm(total=n_batches, desc=pbar_desc)\n",
    "    # Iterate over the batches of texts\n",
    "    for batch in chunk(texts, batch_size):\n",
    "        # Translate the batch of texts\n",
    "        translations += translate_batch_safely(batch, **kwargs)\n",
    "        if verbose: \n",
    "            pbar.update(1)\n",
    "    if verbose: \n",
    "        pbar.close()\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Union, List\n",
    "\n",
    "# helpers\n",
    "def is_string_series(s: pd.Series):\n",
    "    \"\"\"\n",
    "    Test if pandas series is a string series/series of strings\n",
    "    \n",
    "    source: https://stackoverflow.com/a/67001213\n",
    "    \"\"\"\n",
    "    if isinstance(s.dtype, pd.StringDtype):\n",
    "        # The series was explicitly created as a string series (Pandas>=1.0.0)\n",
    "        return True\n",
    "    elif s.dtype == 'object':\n",
    "        # Object series, check each value\n",
    "        return all(isinstance(v, str) or (v is None) or (np.isnan(v)) for v in s)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_nonempty_string(s: pd.Series):\n",
    "    return np.array([isinstance(v, str) and len(v) > 0 for v in s], dtype=bool)\n",
    "\n",
    "# main function\n",
    "def translate_df(\n",
    "        df: pd.DataFrame, \n",
    "        translation_function: Callable,\n",
    "        supported_languages: List[str],\n",
    "        text_col: str = 'text', \n",
    "        lang_col: str = 'lang',\n",
    "        target_language: str = 'en',\n",
    "        target_col: str = 'translation',\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    "        batch_size: int = 16,\n",
    "        verbose: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Translates the texts in a data frame from the source languages specified in a column to a target language and add the translations to the data frame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the texts to be translated.\n",
    "        translation_function (Callable): The translation function to be used.\n",
    "        supported_languages (List[str]): A list language codes supported by the translation model.\n",
    "        text_col (str): The name of the column in the DataFrame that contains the texts to be translated. Default is 'text'.\n",
    "        lang_col (str): The name of the column in the DataFrame that contains the language codes. Default is 'lang'.\n",
    "        target_language (str): The target language to translate the texts to. Can be either an ISO 639-1 or ISO 639-2 language code. Default is 'en'.\n",
    "        target_col (str): The name of the column in the DataFrame to store the translations. Default is 'translation'.\n",
    "        supported_languages (List[str]): A list of ISO 639-1 or ISO 639-2 language codes supported by the translation model. Default is None.\n",
    "        device (Union[str, torch.device]): The device to use for translation. Default is 'cpu' but should be compatible with device used by translation model.\n",
    "        batch_size (int): The size of each translation batch. Default is 16.\n",
    "        **kwargs: Additional keyword arguments to be passed to the `translate_in_batches` function which, in turn, passes them to the translation function.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the translated texts in column `target_col` in the target language `target_lang`.\n",
    "    \"\"\"\n",
    "    # validate the inputs\n",
    "    assert text_col in df.columns, f'Column \"{text_col}\" not found in data frame.'\n",
    "    assert is_string_series(df[text_col]), f'Column \"{text_col}\" is not a series of string values.'\n",
    "    assert lang_col in df.columns, f'Column \"{lang_col}\" not found in data frame.'\n",
    "    assert is_string_series(df[lang_col]), f'Column \"{lang_col}\" is not a series of string values.'\n",
    "    assert target_language is not None, 'Target language must be specified.'\n",
    "    assert target_col not in df.columns, f'Column \"{target_col}\" already exists in data frame.'\n",
    "    assert translation_function is not None, 'Translation function must be specified.'\n",
    "    assert batch_size > 0, 'Batch size must be greater than 0.'\n",
    "    assert supported_languages is not None, 'Supported languages must be specified.'\n",
    "    assert isinstance(supported_languages, list), 'Supported languages must be a list.'\n",
    "    assert len(supported_languages) > 0, 'Supported languages must not be empty.'\n",
    "    assert all([isinstance(l, str) for l in supported_languages]), 'Supported languages must be a list of strings.'\n",
    "    \n",
    "    # check whether the model supports the target language\n",
    "    langs = df['lang'].unique().tolist()\n",
    "    # try to get the ISO 639-1 or ISO 639-2 language code for each language in the data frame\n",
    "    langs_map = {\n",
    "        l: l if iso639.is_valid639_1(l) else iso639.to_iso639_1(l) if iso639.is_valid639_2(l) else None \n",
    "        for l in langs\n",
    "    }\n",
    "    # check whether there are unsupported languages\n",
    "    not_supported = [\n",
    "        l \n",
    "        for l, c in langs_map.items() \n",
    "        if l not in supported_languages and c not in supported_languages and l != target_language and c != target_language\n",
    "    ]\n",
    "    # print warning message if there are unsupported languages\n",
    "    if len(not_supported) > 0:\n",
    "        print(\n",
    "            f'WARNING: values {not_supported} in column \"{lang_col}\" are not supported by NMT model.',\n",
    "            'Texts with these values will not be translated.'\n",
    "        )\n",
    "    # now update language mapping with \"correct\" language codes (use ISO code if available, otherwise use original indicator from the data frame)\n",
    "    langs_map = {\n",
    "        l: c if c in supported_languages else l if l in supported_languages else None \n",
    "        for l, c in langs_map.items()\n",
    "    }\n",
    "\n",
    "    # create new column for translation\n",
    "    df[target_col] = [None]*len(df)\n",
    "\n",
    "    # iterate over languages\n",
    "    for l, d in df.groupby(lang_col):\n",
    "        lang_code = langs_map[l]\n",
    "        # just copy texts if source language is the target language\n",
    "        if lang_code == target_language or l == target_language:\n",
    "            df.loc[d.index, target_col] = d[text_col].tolist()\n",
    "            continue\n",
    "        # skip unsupported languages\n",
    "        if l in not_supported or lang_code is None:\n",
    "            continue\n",
    "        # test for each text value if non-empty string\n",
    "        flag = is_nonempty_string(d[text_col])\n",
    "        if any(~flag):\n",
    "            print(f'WARNING: {sum(~flag)} empty or non-string text(s) in \"{l}\"')\n",
    "        df.loc[d.index[flag], target_col] = translate_in_batches(\n",
    "            texts=d[text_col][flag].tolist(), # <== only translate non-empty texts\n",
    "            translation_fun=translation_function,\n",
    "            device=device,\n",
    "            batch_size=batch_size, \n",
    "            source_lang=lang_code, \n",
    "            target_lang=target_language,\n",
    "            verbose=verbose, \n",
    "            pbar_desc=f'translating {len(d)} text(s) from \"{l}\"',\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def translate_df_with_easynmt(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Translates a DataFrame using the EasyNMT model.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to be translated.\n",
    "        args: Additional arguments for the translation process.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The translated DataFrame.\n",
    "    \"\"\"\n",
    "    args = SimpleNamespace(**kwargs)\n",
    "\n",
    "    try:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "        model = easynmt.EasyNMT(args.model_name, device=device)\n",
    "        print(f'Using device \"{model.device}\"')\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: could not load model \"{args.model_name}\"')\n",
    "        raise e\n",
    "    \n",
    "    tgt_lang = [l.lower() for l in model.get_languages() if args.target_language.lower() == l.lower() or args.target_language.lower() in l.lower()]\n",
    "    if len(tgt_lang) == 0:\n",
    "        raise ValueError(f'Target language \"{args.target_language}\" not supported by DeepL.')\n",
    "    if len(tgt_lang) > 1:\n",
    "        raise ValueError(f'Target language \"{args.target_language}\" ambiguous. Please specify one of {tgt_lang}.')\n",
    "    tgt_lang = tgt_lang[0]\n",
    "    src_langs = model.get_languages(target_lang = tgt_lang)\n",
    "\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    try:\n",
    "        df = translate_df(\n",
    "            df=df, \n",
    "            # data frame arguments\n",
    "            text_col=args.text_col,\n",
    "            lang_col=args.lang_col,\n",
    "            target_language=tgt_lang, \n",
    "            target_col=args.target_col if hasattr(args, 'target_col') else f'{args.text_col}_mt_{args.model_name.lower()}',\n",
    "            # translation model arguments\n",
    "            translation_function=model.translate,\n",
    "            supported_languages=src_langs,\n",
    "            batch_size=args.batch_size if hasattr(args, 'batch_size') else 16,\n",
    "            device=model.device,\n",
    "            # arguments forwarded to model.translate()\n",
    "            beam_size=5,\n",
    "            perform_sentence_splitting=False,\n",
    "            show_progress_bar=False, \n",
    "            # print progress bar\n",
    "            verbose=args.verbose,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: Error during translation \"{str(e)}\". Returning data frame with translations so far.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "def translate_df_with_deepl(df, **kwargs):\n",
    "    args = SimpleNamespace(**kwargs)\n",
    "\n",
    "    # get API key\n",
    "    try:\n",
    "        with open(args.api_key_file) as f:\n",
    "            api_key = f.read().strip()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Could not load API key file \"{args.api_key_file}\". Reason: {str(e)}')\n",
    "        \n",
    "    # initialize a `Translator` instance\n",
    "    try:\n",
    "        translator = deepl.Translator(api_key)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Could not connect to DeepL API. Reason: {str(e)}')\n",
    "\n",
    "    # get source and target languages\n",
    "    src_langs = [l.code.lower() for l in translator.get_source_languages()]\n",
    "    tgt_lang = [l.code.lower() for l in translator.get_target_languages() if args.target_language.lower() == l.code.lower() or args.target_language.lower() in l.code.lower()]\n",
    "    if len(tgt_lang) == 0:\n",
    "        raise ValueError(f'Target language \"{args.target_language}\" not supported by DeepL.')\n",
    "    if len(tgt_lang) > 1:\n",
    "        raise ValueError(f'Target language \"{args.target_language}\" ambiguous. Please specify one of {tgt_lang}.')\n",
    "    tgt_lang = tgt_lang[0]\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    tgt_col = f'{args.text_col}_mt_deepl'\n",
    "    \n",
    "    # translate\n",
    "    try:\n",
    "        df = translate_df(\n",
    "            df=df, \n",
    "            # data frame arguments\n",
    "            text_col=args.text_col,\n",
    "            lang_col=args.lang_col,\n",
    "            target_language=tgt_lang,\n",
    "            target_col=args.target_col if hasattr(args, 'target_col') else tgt_col,\n",
    "            # translation model arguments\n",
    "            translation_function=translator.translate_text,\n",
    "            supported_languages=src_langs,\n",
    "            batch_size=args.batch_size if hasattr(args, 'batch_size') else 128,\n",
    "            # arguments forwarded to translator.translate_text()\n",
    "            split_sentences='off' if not hasattr(args, 'split_sentences') else 'on' if args.split_sentences else 'off',\n",
    "            # print progress bar\n",
    "            verbose=args.verbose,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: Error during translation \"{str(e)}\". Returning data frame with translations so far.')\n",
    "    \n",
    "    try:\n",
    "        # post-process translation result\n",
    "        df[tgt_col] = df[tgt_col].apply(lambda x: x if isinstance(x, str) else x.text if x is not None else None)\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: Error during post-processing \"{str(e)}\". Returning data frame with translations so far.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import translate_v2 as gt\n",
    "\n",
    "def translate_df_with_google(df, **kwargs):\n",
    "    args = SimpleNamespace(**kwargs)\n",
    "\n",
    "    # get API key\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(args.api_key_file)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Could not load API key file \"{args.api_key_file}\". Reason: {str(e)}')\n",
    "    \n",
    "    # initialize a `translator` instance\n",
    "    try:\n",
    "        translator = gt.Client(credentials=credentials)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Could not connect to Google Cloud Translation API. Reason: {str(e)}')\n",
    "    \n",
    "    # get source and target languages\n",
    "    src_langs = [l['language']  for l in translator.get_languages()]\n",
    "    tgt_lang = [l.lower() for l in src_langs if args.target_language.lower() == l.lower() or args.target_language.lower() in l.lower()]\n",
    "    if len(tgt_lang) == 0:\n",
    "        raise ValueError(f'Target language \"{args.target_language}\" not supported by Google Cloud Trsanslation API.')\n",
    "    if len(tgt_lang) > 1:\n",
    "        raise ValueError(f'Target language \"{args.target_language}\" ambiguous. Please specify one of {tgt_lang}.')\n",
    "    tgt_lang = tgt_lang[0]\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    tgt_col = f'{args.text_col}_mt_google'\n",
    "    \n",
    "    def translate_util(values, target_lang, source_lang, **kwargs):\n",
    "        return translator.translate(values=values, target_language=target_lang, source_language=source_lang, **kwargs)\n",
    "\n",
    "    # translate\n",
    "    try:\n",
    "        df = translate_df(\n",
    "            df=df, \n",
    "            # data frame arguments\n",
    "            text_col=args.text_col,\n",
    "            lang_col=args.lang_col,\n",
    "            target_language=tgt_lang,\n",
    "            target_col=args.target_col if hasattr(args, 'target_col') else tgt_col,\n",
    "            # translation model arguments\n",
    "            translation_function=translate_util,\n",
    "            supported_languages=src_langs,\n",
    "            batch_size=args.batch_size if hasattr(args, 'batch_size') else 128,\n",
    "            # print progress bar\n",
    "            verbose=args.verbose,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: Error during translation \"{str(e)}\". Returning data frame with translations so far.')\n",
    "    \n",
    "    try:\n",
    "        # post-process translation result\n",
    "        df[tgt_col] = df[tgt_col].apply(lambda x: x if isinstance(x, str) else x['translatedText'] if x is not None else None)\n",
    "    except Exception as e:\n",
    "        print(f'WARNING: Error during post-processing \"{str(e)}\". Returning data frame with translations so far.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial services\n",
    "\n",
    "**WARNING:** If you run this code, you'll spend ~40 Dollars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate\n",
    "out = translate_df_with_google(\n",
    "    df=df, \n",
    "    api_key_file=os.path.join(os.environ['SPATH'], 'multilingual-gesis-translate.json'),\n",
    "    # data frame arguments\n",
    "    text_col='text',\n",
    "    lang_col='lang',\n",
    "    target_language='en', \n",
    "    target_col='text_mt_google',\n",
    "    batch_size=128,\n",
    "    # print progress bar\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = translate_df_with_deepl(\n",
    "    df=out, \n",
    "    api_key_file=os.path.join(os.environ['SPATH'], 'deepl'),\n",
    "    # data frame arguments\n",
    "    text_col='text',\n",
    "    lang_col='lang',\n",
    "    target_language='en-gb', \n",
    "    target_col='text_mt_deepl',\n",
    "    # print progress bar\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's save some money but spend some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = translate_df_with_easynmt(\n",
    "    df=out, \n",
    "    model_name='m2m_100_418M',\n",
    "    # data frame arguments\n",
    "    text_col='text',\n",
    "    lang_col='lang',\n",
    "    target_language='en', \n",
    "    target_col='text_mt_m2m',\n",
    "    batch_size=8, # <== use small batch size for illustration \n",
    "    # arguments forwarded to model.translate()\n",
    "    beam_size=5,\n",
    "    perform_sentence_splitting=False,\n",
    "    show_progress_bar=False, \n",
    "    # print progress bar\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = translate_df_with_easynmt(\n",
    "    df=out, \n",
    "    model_name='opus-mt',\n",
    "    # data frame arguments\n",
    "    text_col='text',\n",
    "    lang_col='lang',\n",
    "    target_language='en', \n",
    "    target_col='text_mt_opus',\n",
    "    batch_size=8, # <== use small batch size for illustration \n",
    "    # arguments forwarded to model.translate()\n",
    "    beam_size=5,\n",
    "    perform_sentence_splitting=False,\n",
    "    show_progress_bar=False, \n",
    "    # print progress bar\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.to_csv(fp.replace('.tsv', '_translated.tsv'), sep='\\t', index=False)\n",
    "# the resulting file is at https://raw.githubusercontent.com/fabiennelind/Going-Cross-Lingual_Course/main/data/lehmann%2Bzobel_2018_pimpo_positions_translated.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
