{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and pre-processing\n",
    "\n",
    "| Authors | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2023-12-02 |\n",
    "\n",
    "<br>\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/fabiennelind/Going-Cross-Lingual_Course/blob/main/code/preprocessing.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We load generally used libraries here and load the rest on the fly in the respedtive sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "When applying bag-of-words methods to our data, we need to split the text into tokens. \n",
    "\n",
    "The most two popular libraries to handle this that have relatively broad multilingual support are `nltk` and `stanza`\n",
    "\n",
    "**_Note:_** Another library that can tokenize and has [wide language support](https://spacy.io/usage/models#languages) is `spacy` (see [here](https://spacy.io/usage/models) how to use it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hlicht/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizers', 'corpora']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(nltk.data.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['punkt', 'punkt.zip']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(nltk.data.path[0], 'tokenizers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek.pickle',\n",
       " 'estonian.pickle',\n",
       " 'turkish.pickle',\n",
       " '.DS_Store',\n",
       " 'polish.pickle',\n",
       " 'PY3',\n",
       " 'russian.pickle',\n",
       " 'czech.pickle',\n",
       " 'portuguese.pickle',\n",
       " 'README',\n",
       " 'dutch.pickle',\n",
       " 'norwegian.pickle',\n",
       " 'malayalam.pickle',\n",
       " 'slovene.pickle',\n",
       " 'english.pickle',\n",
       " 'danish.pickle',\n",
       " 'finnish.pickle',\n",
       " 'swedish.pickle',\n",
       " 'spanish.pickle',\n",
       " 'german.pickle',\n",
       " 'italian.pickle',\n",
       " 'french.pickle']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(nltk.data.path[0], 'tokenizers', 'punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(nltk.data.path[0], 'tokenizers', 'punkt')\n",
    "nltk_tokenizer_language_support = [f.replace('.pickle', '') for f in os.listdir(fp) if f.endswith('.pickle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greek': 'gre',\n",
       " 'estonian': 'est',\n",
       " 'turkish': 'tur',\n",
       " 'polish': 'pol',\n",
       " 'russian': 'rus',\n",
       " 'czech': 'cze',\n",
       " 'portuguese': 'por',\n",
       " 'dutch': 'dut',\n",
       " 'norwegian': 'nor',\n",
       " 'malayalam': 'mal',\n",
       " 'slovene': 'slv',\n",
       " 'english': 'eng',\n",
       " 'danish': 'dan',\n",
       " 'finnish': 'fin',\n",
       " 'swedish': 'swe',\n",
       " 'spanish': 'spa',\n",
       " 'german': 'ger',\n",
       " 'italian': 'ita',\n",
       " 'french': 'fre'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import iso639\n",
    "\n",
    "{l: iso639.to_iso639_2(l) for l in nltk_tokenizer_language_support}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"\n",
    "greek\tgre\tΗ εξερεύνηση της ανθρώπινης φύσης είναι μια ατέλειωτη περιπέτεια που μας καθοδηγεί σε νέες εννοιολογικές διαδρομές.\n",
    "estonian\test\tEesti rannikul jalutades võib tunnetada mere lõpmatust ja ajalooliste jutustuste rikkust.\n",
    "turkish\ttur\tAnadolu'nun mistik atmosferi, tarih ve kültürün harmanlandığı bir yerdir.\n",
    "polish\tpol\tCiekawość i odkrywanie nowych koncepcji stanowią fundament naszej ludzkiej egzystencji.\n",
    "russian\trus\tРусская литература открывает перед нами богатство человеческого опыта и погружает нас в глубины человеческой души.\n",
    "czech\tcze\tČeská krajina oplývá malebnými zákoutími a historickými památkami, které vyprávějí příběhy minulosti.\n",
    "portuguese\tpor\tA diversidade cultural do Brasil reflete-se na fusão de influências indígenas, africanas e europeias, criando uma rica tapeçaria de tradições.\n",
    "dutch\tdut\tDe grachten van Amsterdam getuigen van een rijke geschiedenis en vormen een intrigerend netwerk dat de stad doorkruist.\n",
    "norwegian\tnor\tNorges majestetiske fjorder og bortgjemte fjelltopper lokker eventyrlystne reisende til å utforske naturens underverker.\n",
    "malayalam\tmal\tകേരളത്തിന്റെ സൗന്ദര്യം പ്രകടമാക്കുന്ന അക്കാദമിക പരിസ്ഥിതികൾ ഹിമാലയത്തിന്റെ സന്ദർശനത്തിന് കേന്ദ്രീകരിക്കുന്നു.\n",
    "slovene\tslv\tSlovenski jezik je bogat s svojimi idiomatskimi izrazi, ki odražajo duhovito naravo in globoko čustvenost.\n",
    "english\teng\tDelving into the intricacies of quantum mechanics allows us to grasp the profound mysteries that govern the fundamental aspects of the universe.\n",
    "danish\tdan\tDen danske hygge kultur skaber en atmosfære af varme og samhørighed, der omfavner livets enkle glæder.\n",
    "finnish\tfin\tSuomen lumiset maisemat tarjoavat unohtumattoman näkymän pohjoisen luonnon kauneudesta.\n",
    "swedish\tswe\tSverige är känt för sin designinnovation och det sätt på vilket den integreras i vardagen, vilket skapar en harmonisk livsstil.\n",
    "spanish\tspa\tLa arquitectura gótica de la catedral de Barcelona es un testimonio impresionante de la destreza artística y la devoción religiosa.\n",
    "german\tger\tDie deutsche Philosophie hat einen tiefgreifenden Einfluss auf das Denken und die intellektuelle Tradition weltweit ausgeübt.\n",
    "italian\tita\tL'arte culinaria italiana è un'esperienza sensoriale che celebra l'amore per gli ingredienti freschi e la convivialità.\n",
    "french\tfre\tL'effervescence culturelle de Paris, avec ses musées, ses théâtres et ses cafés, fait de la ville une destination artistique incontournable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [row.split('\\t') for row in  sents.split('\\n') if row != '']\n",
    "sents_df = pd.DataFrame(sents, columns=['language', 'code', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greek: \"Η εξερεύνηση της ανθρώπινης φύσης είναι μια ατέλειωτη περιπέτεια που μας καθοδηγεί σε νέες εννοιολογικές διαδρομές.\"\n",
      "['Η', 'εξερεύνηση', 'της', 'ανθρώπινης', 'φύσης', 'είναι', 'μια', 'ατέλειωτη', 'περιπέτεια', 'που', 'μας', 'καθοδηγεί', 'σε', 'νέες', 'εννοιολογικές', 'διαδρομές', '.']\n",
      "\n",
      "estonian: \"Eesti rannikul jalutades võib tunnetada mere lõpmatust ja ajalooliste jutustuste rikkust.\"\n",
      "['Eesti', 'rannikul', 'jalutades', 'võib', 'tunnetada', 'mere', 'lõpmatust', 'ja', 'ajalooliste', 'jutustuste', 'rikkust', '.']\n",
      "\n",
      "turkish: \"Anadolu'nun mistik atmosferi, tarih ve kültürün harmanlandığı bir yerdir.\"\n",
      "[\"Anadolu'nun\", 'mistik', 'atmosferi', ',', 'tarih', 've', 'kültürün', 'harmanlandığı', 'bir', 'yerdir', '.']\n",
      "\n",
      "polish: \"Ciekawość i odkrywanie nowych koncepcji stanowią fundament naszej ludzkiej egzystencji.\"\n",
      "['Ciekawość', 'i', 'odkrywanie', 'nowych', 'koncepcji', 'stanowią', 'fundament', 'naszej', 'ludzkiej', 'egzystencji', '.']\n",
      "\n",
      "russian: \"Русская литература открывает перед нами богатство человеческого опыта и погружает нас в глубины человеческой души.\"\n",
      "['Русская', 'литература', 'открывает', 'перед', 'нами', 'богатство', 'человеческого', 'опыта', 'и', 'погружает', 'нас', 'в', 'глубины', 'человеческой', 'души', '.']\n",
      "\n",
      "czech: \"Česká krajina oplývá malebnými zákoutími a historickými památkami, které vyprávějí příběhy minulosti.\"\n",
      "['Česká', 'krajina', 'oplývá', 'malebnými', 'zákoutími', 'a', 'historickými', 'památkami', ',', 'které', 'vyprávějí', 'příběhy', 'minulosti', '.']\n",
      "\n",
      "portuguese: \"A diversidade cultural do Brasil reflete-se na fusão de influências indígenas, africanas e europeias, criando uma rica tapeçaria de tradições.\"\n",
      "['A', 'diversidade', 'cultural', 'do', 'Brasil', 'reflete-se', 'na', 'fusão', 'de', 'influências', 'indígenas', ',', 'africanas', 'e', 'europeias', ',', 'criando', 'uma', 'rica', 'tapeçaria', 'de', 'tradições', '.']\n",
      "\n",
      "dutch: \"De grachten van Amsterdam getuigen van een rijke geschiedenis en vormen een intrigerend netwerk dat de stad doorkruist.\"\n",
      "['De', 'grachten', 'van', 'Amsterdam', 'getuigen', 'van', 'een', 'rijke', 'geschiedenis', 'en', 'vormen', 'een', 'intrigerend', 'netwerk', 'dat', 'de', 'stad', 'doorkruist', '.']\n",
      "\n",
      "norwegian: \"Norges majestetiske fjorder og bortgjemte fjelltopper lokker eventyrlystne reisende til å utforske naturens underverker.\"\n",
      "['Norges', 'majestetiske', 'fjorder', 'og', 'bortgjemte', 'fjelltopper', 'lokker', 'eventyrlystne', 'reisende', 'til', 'å', 'utforske', 'naturens', 'underverker', '.']\n",
      "\n",
      "malayalam: \"കേരളത്തിന്റെ സൗന്ദര്യം പ്രകടമാക്കുന്ന അക്കാദമിക പരിസ്ഥിതികൾ ഹിമാലയത്തിന്റെ സന്ദർശനത്തിന് കേന്ദ്രീകരിക്കുന്നു.\"\n",
      "['കേരളത്തിന്റെ', 'സൗന്ദര്യം', 'പ്രകടമാക്കുന്ന', 'അക്കാദമിക', 'പരിസ്ഥിതികൾ', 'ഹിമാലയത്തിന്റെ', 'സന്ദർശനത്തിന്', 'കേന്ദ്രീകരിക്കുന്നു', '.']\n",
      "\n",
      "slovene: \"Slovenski jezik je bogat s svojimi idiomatskimi izrazi, ki odražajo duhovito naravo in globoko čustvenost.\"\n",
      "['Slovenski', 'jezik', 'je', 'bogat', 's', 'svojimi', 'idiomatskimi', 'izrazi', ',', 'ki', 'odražajo', 'duhovito', 'naravo', 'in', 'globoko', 'čustvenost', '.']\n",
      "\n",
      "english: \"Delving into the intricacies of quantum mechanics allows us to grasp the profound mysteries that govern the fundamental aspects of the universe.\"\n",
      "['Delving', 'into', 'the', 'intricacies', 'of', 'quantum', 'mechanics', 'allows', 'us', 'to', 'grasp', 'the', 'profound', 'mysteries', 'that', 'govern', 'the', 'fundamental', 'aspects', 'of', 'the', 'universe', '.']\n",
      "\n",
      "danish: \"Den danske hygge kultur skaber en atmosfære af varme og samhørighed, der omfavner livets enkle glæder.\"\n",
      "['Den', 'danske', 'hygge', 'kultur', 'skaber', 'en', 'atmosfære', 'af', 'varme', 'og', 'samhørighed', ',', 'der', 'omfavner', 'livets', 'enkle', 'glæder', '.']\n",
      "\n",
      "finnish: \"Suomen lumiset maisemat tarjoavat unohtumattoman näkymän pohjoisen luonnon kauneudesta.\"\n",
      "['Suomen', 'lumiset', 'maisemat', 'tarjoavat', 'unohtumattoman', 'näkymän', 'pohjoisen', 'luonnon', 'kauneudesta', '.']\n",
      "\n",
      "swedish: \"Sverige är känt för sin designinnovation och det sätt på vilket den integreras i vardagen, vilket skapar en harmonisk livsstil.\"\n",
      "['Sverige', 'är', 'känt', 'för', 'sin', 'designinnovation', 'och', 'det', 'sätt', 'på', 'vilket', 'den', 'integreras', 'i', 'vardagen', ',', 'vilket', 'skapar', 'en', 'harmonisk', 'livsstil', '.']\n",
      "\n",
      "spanish: \"La arquitectura gótica de la catedral de Barcelona es un testimonio impresionante de la destreza artística y la devoción religiosa.\"\n",
      "['La', 'arquitectura', 'gótica', 'de', 'la', 'catedral', 'de', 'Barcelona', 'es', 'un', 'testimonio', 'impresionante', 'de', 'la', 'destreza', 'artística', 'y', 'la', 'devoción', 'religiosa', '.']\n",
      "\n",
      "german: \"Die deutsche Philosophie hat einen tiefgreifenden Einfluss auf das Denken und die intellektuelle Tradition weltweit ausgeübt.\"\n",
      "['Die', 'deutsche', 'Philosophie', 'hat', 'einen', 'tiefgreifenden', 'Einfluss', 'auf', 'das', 'Denken', 'und', 'die', 'intellektuelle', 'Tradition', 'weltweit', 'ausgeübt', '.']\n",
      "\n",
      "italian: \"L'arte culinaria italiana è un'esperienza sensoriale che celebra l'amore per gli ingredienti freschi e la convivialità.\"\n",
      "[\"L'arte\", 'culinaria', 'italiana', 'è', \"un'esperienza\", 'sensoriale', 'che', 'celebra', \"l'amore\", 'per', 'gli', 'ingredienti', 'freschi', 'e', 'la', 'convivialità', '.']\n",
      "\n",
      "french: \"L'effervescence culturelle de Paris, avec ses musées, ses théâtres et ses cafés, fait de la ville une destination artistique incontournable.\"\n",
      "[\"L'effervescence\", 'culturelle', 'de', 'Paris', ',', 'avec', 'ses', 'musées', ',', 'ses', 'théâtres', 'et', 'ses', 'cafés', ',', 'fait', 'de', 'la', 'ville', 'une', 'destination', 'artistique', 'incontournable', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create word tokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for i, d in sents_df.iterrows():\n",
    "    print(f'{d.language}: \"{d.text}\"')\n",
    "    print(word_tokenize(d.text, language=d.language))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `stanza`\n",
    "\n",
    "supported languages are listed here: https://stanfordnlp.github.io/stanza/performance.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sl',\n",
       " 'sk',\n",
       " 'ur',\n",
       " 'zh-hans',\n",
       " 'kmr',\n",
       " 'zh-hant',\n",
       " 'ug',\n",
       " 'pl',\n",
       " 'bxr',\n",
       " 'vi',\n",
       " 'fro',\n",
       " 'sv',\n",
       " 'ga',\n",
       " 'he',\n",
       " 'mt',\n",
       " 'qaf',\n",
       " 'hy',\n",
       " 'nn',\n",
       " 'be',\n",
       " 'da',\n",
       " 'mr',\n",
       " 'kk',\n",
       " 'ky',\n",
       " 'grc',\n",
       " 'ja',\n",
       " 'cu',\n",
       " 'el',\n",
       " 'lv',\n",
       " 'it',\n",
       " 'ca',\n",
       " 'is',\n",
       " 'cs',\n",
       " 'te',\n",
       " 'ru',\n",
       " 'got',\n",
       " 'resources.json',\n",
       " 'ro',\n",
       " 'hsb',\n",
       " 'hyw',\n",
       " 'sa',\n",
       " 'pt',\n",
       " 'qpm',\n",
       " 'uk',\n",
       " 'sr',\n",
       " 'pcm',\n",
       " 'lij',\n",
       " 'ar',\n",
       " 'gl',\n",
       " 'gv',\n",
       " 'hr',\n",
       " 'hu',\n",
       " 'nl',\n",
       " 'bg',\n",
       " 'myv',\n",
       " 'af',\n",
       " 'nb',\n",
       " 'hi',\n",
       " 'de',\n",
       " 'sme',\n",
       " 'gd',\n",
       " 'ko',\n",
       " 'fi',\n",
       " 'orv',\n",
       " 'id',\n",
       " 'fr',\n",
       " 'es',\n",
       " 'et',\n",
       " 'en',\n",
       " 'fa',\n",
       " 'lt',\n",
       " 'fo',\n",
       " 'cy',\n",
       " 'eu',\n",
       " 'hbo',\n",
       " 'la',\n",
       " 'qtd',\n",
       " 'ta',\n",
       " 'lzh',\n",
       " 'tr',\n",
       " 'cop',\n",
       " 'wo']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list tokenizer resources for stanza\n",
    "import os\n",
    "os.listdir(os.path.join(os.path.expanduser('~'), 'stanza_resources'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ancient Greek</td>\n",
       "      <td>grc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ancient Hebrew</td>\n",
       "      <td>hbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenian</td>\n",
       "      <td>hy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Uyghur</td>\n",
       "      <td>ug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Welsh</td>\n",
       "      <td>cy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Western Armenian</td>\n",
       "      <td>hyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Wolof</td>\n",
       "      <td>wo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            language code\n",
       "0          Afrikaans   af\n",
       "1      Ancient Greek  grc\n",
       "2     Ancient Hebrew  hbo\n",
       "3             Arabic   ar\n",
       "4           Armenian   hy\n",
       "..               ...  ...\n",
       "75            Uyghur   ug\n",
       "76        Vietnamese   vi\n",
       "77             Welsh   cy\n",
       "78  Western Armenian  hyw\n",
       "79             Wolof   wo\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 'https://github.com/fabiennelind/Going-Cross-Lingual_Course/blob/main/resources/stanza_tokenization_language_support.tsv'\n",
    "pd.read_csv(fp, sep='\\t'   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"\n",
    "Afrikaans\taf\tEk hou van om in die natuur te wandel.\n",
    "Ancient Greek\tgrc\tΧαίρετε, ὦ κόσμε!\n",
    "Ancient Hebrew\thbo\tשָׁלוֹם לָכֶם\n",
    "Arabic\tar\tالسلام عليكم ورحمة الله وبركاته\n",
    "Armenian\thy\tԲարեւ, աշխարհ!\n",
    "Basque\teu\tKaixo mundua!\n",
    "Belarusian\tbe\tПрывітанне, свет!\n",
    "Bulgarian\tbg\tЗдравейте, свят!\n",
    "Buryat\tbxr\tСайн байна уу?\n",
    "Catalan\tca\tHola, món!\n",
    "Chinese (Simplified)\tzh-hans\t你好，世界！\n",
    "Chinese (Traditional)\tzh-hant\t你好，世界！\n",
    "Classical Chinese\tlzh\t世界安好\n",
    "Coptic\tcop\tϨⲟⲩⲧⲁⲧⲏⲣⲓⲟⲛⲁⲧⲓ ϭⲟⲟⲩⲧ ⲛ̀ⲧⲉⲛⲁⲩⲧⲏⲣ\n",
    "Croatian\thr\tPozdrav, svijete!\n",
    "Czech\tcs\tAhoj, světe!\n",
    "Danish\tda\tHej verden!\n",
    "Dutch\tnl\tHallo, wereld!\n",
    "English\ten\tHello, world!\n",
    "Erzya\tmyv\tТевеме варянтись!\n",
    "Estonian\tet\tTere, maailm!\n",
    "Faroese\tfo\tHalló, heimur!\n",
    "Finnish\tfi\tHei, maailma!\n",
    "French\tfr\tBonjour, le monde !\n",
    "Galician\tgl\tOla, mundo!\n",
    "German\tde\tHallo, Welt!\n",
    "Gothic\tgot\t𐌷𐌰𐌽𐍃, 𐌰𐌷𐍄𐌹𐌿𐍃!\n",
    "Greek\tel\tΓεια σας, κόσμε!\n",
    "Hebrew\the\tשלום עולם\n",
    "Hindi\thi\tनमस्ते, दुनिया!\n",
    "Hungarian\thu\tHelló, világ!\n",
    "Icelandic\tis\tHalló, heimur!\n",
    "Indonesian\tid\tHalo, dunia!\n",
    "Irish\tga\tDia dhuit, domhan!\n",
    "Italian\tit\tCiao, mondo!\n",
    "Japanese\tja\tこんにちは、世界！\n",
    "Kazakh\tkk\tСәлеметсіздерге, өткен!\n",
    "Korean\tko\t안녕하세요, 세계!\n",
    "Kurmanji\tkmr\tSilav, cîhan!\n",
    "Kyrgyz\tky\tСаламатсыздарга, дүйнө!\n",
    "Latin\tla\tSalve, mundus!\n",
    "Latvian\tlv\tSveiki, pasaule!\n",
    "Ligurian\tlij\tCiao, mondo!\n",
    "Lithuanian\tlt\tLabas, pasauli!\n",
    "Maghrebi Arabic French\tqaf\tالسلام عليكم، عالم!\n",
    "Maltese\tmt\tBongu, dinja!\n",
    "Manx\tgv\tLaa rieau, yn cheer!\n",
    "Marathi\tmr\tहॅलो, जग!\n",
    "Naija\tpcm\tHow far, world!\n",
    "North Sami\tsme\tČázeheaddji, oktavuohta!\n",
    "Norwegian\tnb\tHei, verden!\n",
    "Norwegian Nynorsk\tnn\tHei, verda!\n",
    "Old Church Slavonic\tcu\tЗдрaвѣй, свѣтѣ!\n",
    "Old East Slavic\torv\tЗдрaвѣй, миръ!\n",
    "Old French\tfro\tSalut, monde!\n",
    "Persian\tfa\tسلام، جهان!\n",
    "Polish\tpl\tWitaj, świecie!\n",
    "Pomak\tqpm\tЗдравей, свят!\n",
    "Portuguese\tpt\tOlá, mundo!\n",
    "Romanian\tro\tSalut, lume!\n",
    "Russian\tru\tПривет, мир!\n",
    "Sanskrit\tsa\tनमस्ते, लोक!\n",
    "Scottish Gaelic\tgd\tHàlo, saoghal!\n",
    "Serbian\tsr\tЗдраво, свете!\n",
    "Slovak\tsk\tAhoj, svet!\n",
    "Slovenian\tsl\tZdravo, svet!\n",
    "Spanish\tes\t¡Hola, mundo!\n",
    "Swedish\tsv\tHej, världen!\n",
    "Tamil\tta\tவணக்கம், உலகே!\n",
    "Telugu\tte\tహలో, ప్రపంచం!\n",
    "Turkish\ttr\tMerhaba, dünya!\n",
    "Turkish German\tqtd\tMerhaba, dünya!\n",
    "Ukrainian\tuk\tПривіт, світ!\n",
    "Upper Sorbian\thsb\tHellos, swět!\n",
    "Urdu\tur\tہیلو، دنیا!\n",
    "Uyghur\tug\tھەلەلۇ، دۇنيا!\n",
    "Vietnamese\tvi\tChào bạn, thế giới!\n",
    "Welsh\tcy\tHelo, byd!\n",
    "Western Armenian\thyw\tՈղջույն, աշխարհ!\n",
    "Wolof\two\tNopp naa ngi ci biir!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [row.split('\\t') for row in  sents.split('\\n') if row != '']\n",
    "sents_df = pd.DataFrame(sents, columns=['language', 'code', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans: \"Ek hou van om in die natuur te wandel.\"\n",
      "['Ek', 'hou', 'van', 'om', 'in', 'die', 'natuur', 'te', 'wandel', '.']\n",
      "\n",
      "Ancient Greek: \"Χαίρετε, ὦ κόσμε!\"\n",
      "['Χαίρετε,', 'ὦ', 'κόσμε!']\n",
      "\n",
      "Ancient Hebrew: \"שָׁלוֹם לָכֶם\"\n",
      "['שָׁלוֹם', 'לָכֶם']\n",
      "\n",
      "Arabic: \"السلام عليكم ورحمة الله وبركاته\"\n",
      "['السلام', 'عليكم', 'ورحمة', 'الله', 'وبركاته']\n",
      "\n",
      "Armenian: \"Բարեւ, աշխարհ!\"\n",
      "['Բարեւ', ',', 'աշխարհ!']\n",
      "\n",
      "Basque: \"Kaixo mundua!\"\n",
      "['Kaixo', 'mundua', '!']\n",
      "\n",
      "Belarusian: \"Прывітанне, свет!\"\n",
      "['Прывітанне', ',', 'свет', '!']\n",
      "\n",
      "Bulgarian: \"Здравейте, свят!\"\n",
      "['Здравейте', ',', 'свят', '!']\n",
      "\n",
      "Buryat: \"Сайн байна уу?\"\n",
      "['Сайн', 'байна', 'уу', '?']\n",
      "\n",
      "Catalan: \"Hola, món!\"\n",
      "['Hola', ',', 'món', '!']\n",
      "\n",
      "Chinese (Simplified): \"你好，世界！\"\n",
      "['你好', '，', '世界', '！']\n",
      "\n",
      "Chinese (Traditional): \"你好，世界！\"\n",
      "['你', '好', '，', '世界', '！']\n",
      "\n",
      "Classical Chinese: \"世界安好\"\n",
      "['世', '界', '安', '好']\n",
      "\n",
      "Coptic: \"Ϩⲟⲩⲧⲁⲧⲏⲣⲓⲟⲛⲁⲧⲓ ϭⲟⲟⲩⲧ ⲛ̀ⲧⲉⲛⲁⲩⲧⲏⲣ\"\n",
      "['Ϩⲟⲩⲧⲁⲧⲏⲣⲓⲟⲛⲁⲧⲓ', 'ϭⲟⲟⲩⲧ', 'ⲛ̀ⲧⲉⲛⲁⲩⲧⲏⲣ']\n",
      "\n",
      "Croatian: \"Pozdrav, svijete!\"\n",
      "['Pozdrav', ',', 'svijete', '!']\n",
      "\n",
      "Czech: \"Ahoj, světe!\"\n",
      "['Ahoj', ',', 'světe', '!']\n",
      "\n",
      "Danish: \"Hej verden!\"\n",
      "['Hej', 'verden', '!']\n",
      "\n",
      "Dutch: \"Hallo, wereld!\"\n",
      "['Hallo', ',', 'wereld', '!']\n",
      "\n",
      "English: \"Hello, world!\"\n",
      "['Hello', ',', 'world', '!']\n",
      "\n",
      "Erzya: \"Тевеме варянтись!\"\n",
      "['Тевеме', 'варянтись', '!']\n",
      "\n",
      "Estonian: \"Tere, maailm!\"\n",
      "['Tere', ',', 'maailm', '!']\n",
      "\n",
      "Faroese: \"Halló, heimur!\"\n",
      "['Halló', ',', 'heimur', '!']\n",
      "\n",
      "Finnish: \"Hei, maailma!\"\n",
      "['Hei', ',', 'maailma', '!']\n",
      "\n",
      "French: \"Bonjour, le monde !\"\n",
      "['Bonjour', ',', 'le', 'monde', '!']\n",
      "\n",
      "Galician: \"Ola, mundo!\"\n",
      "['Ola', ',', 'mundo', '!']\n",
      "\n",
      "German: \"Hallo, Welt!\"\n",
      "['Hallo', ',', 'Welt', '!']\n",
      "\n",
      "Gothic: \"𐌷𐌰𐌽𐍃, 𐌰𐌷𐍄𐌹𐌿𐍃!\"\n",
      "['𐌷𐌰𐌽𐍃,', '𐌰𐌷𐍄𐌹𐌿𐍃', '!']\n",
      "\n",
      "Greek: \"Γεια σας, κόσμε!\"\n",
      "['Γεια', 'σας', ',', 'κόσμε', '!']\n",
      "\n",
      "Hebrew: \"שלום עולם\"\n",
      "['שלום', 'עולם']\n",
      "\n",
      "Hindi: \"नमस्ते, दुनिया!\"\n",
      "['नमस्ते', ',', 'दुनिया', '!']\n",
      "\n",
      "Hungarian: \"Helló, világ!\"\n",
      "['Helló', ',', 'világ', '!']\n",
      "\n",
      "Icelandic: \"Halló, heimur!\"\n",
      "['Halló', ',', 'heimur', '!']\n",
      "\n",
      "Indonesian: \"Halo, dunia!\"\n",
      "['Halo', ',', 'dunia', '!']\n",
      "\n",
      "Irish: \"Dia dhuit, domhan!\"\n",
      "['Dia', 'dhuit', ',', 'domhan', '!']\n",
      "\n",
      "Italian: \"Ciao, mondo!\"\n",
      "['Ciao', ',', 'mondo', '!']\n",
      "\n",
      "Japanese: \"こんにちは、世界！\"\n",
      "['こん', 'にち', 'は', '、', '世界', '！']\n",
      "\n",
      "Kazakh: \"Сәлеметсіздерге, өткен!\"\n",
      "['Сәлеметсіздерге', ',', 'өткен', '!']\n",
      "\n",
      "Korean: \"안녕하세요, 세계!\"\n",
      "['안녕하세요', ',', '세계', '!']\n",
      "\n",
      "Kurmanji: \"Silav, cîhan!\"\n",
      "['Silav', ',', 'cîhan', '!']\n",
      "\n",
      "Kyrgyz: \"Саламатсыздарга, дүйнө!\"\n",
      "['Саламатсыздарга', ',', 'дүйнө', '!']\n",
      "\n",
      "Latin: \"Salve, mundus!\"\n",
      "['Salve', ',', 'mundus', '!']\n",
      "\n",
      "Latvian: \"Sveiki, pasaule!\"\n",
      "['Sveiki', ',', 'pasaule', '!']\n",
      "\n",
      "Ligurian: \"Ciao, mondo!\"\n",
      "['Ciao', ',', 'mondo', '!']\n",
      "\n",
      "Lithuanian: \"Labas, pasauli!\"\n",
      "['Labas', ',', 'pasauli', '!']\n",
      "\n",
      "Maghrebi Arabic French: \"السلام عليكم، عالم!\"\n",
      "['السلام', 'عليكم،', 'عالم!']\n",
      "\n",
      "Maltese: \"Bongu, dinja!\"\n",
      "['Bongu', ',', 'dinja', '!']\n",
      "\n",
      "Manx: \"Laa rieau, yn cheer!\"\n",
      "['Laa', 'rieau', ',', 'yn', 'cheer', '!']\n",
      "\n",
      "Marathi: \"हॅलो, जग!\"\n",
      "['हॅलो', ',', 'जग', '!']\n",
      "\n",
      "Naija: \"How far, world!\"\n",
      "['How', 'far,', 'world!']\n",
      "\n",
      "North Sami: \"Čázeheaddji, oktavuohta!\"\n",
      "['Čázeheaddji', ',', 'oktavuohta', '!']\n",
      "\n",
      "Norwegian: \"Hei, verden!\"\n",
      "['Hei', ',', 'verden', '!']\n",
      "\n",
      "Norwegian Nynorsk: \"Hei, verda!\"\n",
      "['Hei', ',', 'verda', '!']\n",
      "\n",
      "Old Church Slavonic: \"Здрaвѣй, свѣтѣ!\"\n",
      "['Здрaвѣй,', 'свѣтѣ!']\n",
      "\n",
      "Old East Slavic: \"Здрaвѣй, миръ!\"\n",
      "['Здрaвѣй,', 'миръ', '!']\n",
      "\n",
      "Old French: \"Salut, monde!\"\n",
      "['Salut', ',', 'monde!']\n",
      "\n",
      "Persian: \"سلام، جهان!\"\n",
      "['سلام', '،', 'جهان', '!']\n",
      "\n",
      "Polish: \"Witaj, świecie!\"\n",
      "['Witaj', ',', 'świecie', '!']\n",
      "\n",
      "Pomak: \"Здравей, свят!\"\n",
      "['Здравей', ',', 'свят', '!']\n",
      "\n",
      "Portuguese: \"Olá, mundo!\"\n",
      "['Olá', ',', 'mundo', '!']\n",
      "\n",
      "Romanian: \"Salut, lume!\"\n",
      "['Salut', ',', 'lume', '!']\n",
      "\n",
      "Russian: \"Привет, мир!\"\n",
      "['Привет', ',', 'мир', '!']\n",
      "\n",
      "Sanskrit: \"नमस्ते, लोक!\"\n",
      "['नमस्ते,', 'लोक!']\n",
      "\n",
      "Scottish Gaelic: \"Hàlo, saoghal!\"\n",
      "['Hàlo', ',', 'saoghal', '!']\n",
      "\n",
      "Serbian: \"Здраво, свете!\"\n",
      "['Здраво', ',', 'свете', '!']\n",
      "\n",
      "Slovak: \"Ahoj, svet!\"\n",
      "['Ahoj', ',', 'svet', '!']\n",
      "\n",
      "Slovenian: \"Zdravo, svet!\"\n",
      "['Zdravo', ',', 'svet', '!']\n",
      "\n",
      "Spanish: \"¡Hola, mundo!\"\n",
      "['¡', 'Hola', ',', 'mundo', '!']\n",
      "\n",
      "Swedish: \"Hej, världen!\"\n",
      "['Hej', ',', 'världen', '!']\n",
      "\n",
      "Tamil: \"வணக்கம், உலகே!\"\n",
      "['வணக்கம்', ',', 'உலகே!']\n",
      "\n",
      "Telugu: \"హలో, ప్రపంచం!\"\n",
      "['హలో', ',', 'ప్రపంచం', '!']\n",
      "\n",
      "Turkish: \"Merhaba, dünya!\"\n",
      "['Merhaba', ',', 'dünya', '!']\n",
      "\n",
      "Turkish German: \"Merhaba, dünya!\"\n",
      "['Merhaba', ',', 'dünya', '!']\n",
      "\n",
      "Ukrainian: \"Привіт, світ!\"\n",
      "['Привіт', ',', 'світ', '!']\n",
      "\n",
      "Upper Sorbian: \"Hellos, swět!\"\n",
      "['Hellos', ',', 'swět', '!']\n",
      "\n",
      "Urdu: \"ہیلو، دنیا!\"\n",
      "['ہیلو', '،', 'دنیا', '!']\n",
      "\n",
      "Uyghur: \"ھەلەلۇ، دۇنيا!\"\n",
      "['ھەلەلۇ', '،', 'دۇنيا', '!']\n",
      "\n",
      "Vietnamese: \"Chào bạn, thế giới!\"\n",
      "['Chào', 'bạ', 'n,', 'thế giới', '!']\n",
      "\n",
      "Welsh: \"Helo, byd!\"\n",
      "['Helo', ',', 'byd', '!']\n",
      "\n",
      "Western Armenian: \"Ողջույն, աշխարհ!\"\n",
      "['Ողջույն', ',', 'աշխարհ', '!']\n",
      "\n",
      "Wolof: \"Nopp naa ngi ci biir!\"\n",
      "['Nopp', 'naa', 'ngi', 'ci', 'biir', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "for i, d in sents_df.iterrows():\n",
    "    print(f'{d.language}: \"{d.text}\"')\n",
    "    doc = stanza.Pipeline(lang=d.code, processors='tokenize', verbose=False)(d.text)\n",
    "    print([tok.text for sent in doc.sentences for tok in sent.tokens])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenization/segmentation/splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a sentence-tokenized copy of *text*,\n",
      "using NLTK's recommended sentence tokenizer\n",
      "(currently :class:`.PunktSentenceTokenizer`\n",
      "for the specified language).\n",
      "\n",
      ":param text: text to split into sentences\n",
      ":param language: the model name in the Punkt corpus\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/multilingual/lib/python3.10/site-packages/nltk/tokenize/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.', 'This is the second sentence.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sentences = 'This is the first sentence. This is the second sentence.'\n",
    "\n",
    "sent_tokenize(two_sentences, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Esta es la primera frase.', 'Esta es la segunda frase.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sentences = 'Esta es la primera frase. Esta es la segunda frase.'\n",
    "\n",
    "sent_tokenize(two_sentences, language='spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence, with a Mr. Smith.',\n",
       " 'This is the second sentence.',\n",
       " 'And now the hon.',\n",
       " 'Gentleman will speak.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_complicated_sentences = 'This is the first sentence, with a Mr. Smith. This is the second sentence. And now the hon. Gentleman will speak.'\n",
    "sent_tokenize(two_complicated_sentences, language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_:** there is a solution to add custom rules to [prevent such errors](https://stackoverflow.com/a/25375857) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿Cuántas veces, en un día típico, te detienes a reflexionar sobre la complejidad intrínseca de la existencia humana, con sus interconexiones profundas y su capacidad infinita para la sorpresa y la maravilla?',\n",
       " 'A medida que exploramos las vastas extensiones del conocimiento, ¿no te parece fascinante cómo se entrelazan las múltiples disciplinas científicas, desde la física cuántica hasta la biología molecular, revelando un universo de posibilidades aún por descubrir?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_complicated_sentences = (\n",
    "    '¿Cuántas veces, en un día típico, te detienes a reflexionar sobre la complejidad intrínseca de la existencia humana, con sus interconexiones profundas y su capacidad infinita para la sorpresa y la maravilla?'\n",
    "    ' '\n",
    "    'A medida que exploramos las vastas extensiones del conocimiento, ¿no te parece fascinante cómo se entrelazan las múltiples disciplinas científicas, desde la física cuántica hasta la biología molecular, revelando un universo de posibilidades aún por descubrir?'\n",
    ")\n",
    "sent_tokenize(two_complicated_sentences, language='spanish')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `stanza`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence.', 'This is the second sentence.']\n",
      "['This is the first sentence, with a Mr. Smith.', 'This is the second sentence.', 'And now the hon.', 'Gentleman will speak.']\n"
     ]
    }
   ],
   "source": [
    "# english\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', verbose=False)\n",
    "\n",
    "two_sentences = 'This is the first sentence. This is the second sentence.'\n",
    "doc = nlp(two_sentences)\n",
    "print([sent.text for sent in doc.sentences])\n",
    "\n",
    "two_complicated_sentences = 'This is the first sentence, with a Mr. Smith. This is the second sentence. And now the hon. Gentleman will speak.'\n",
    "doc = nlp(two_complicated_sentences)\n",
    "print([sent.text for sent in doc.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** There seems to be [a way to handle](https://github.com/stanfordnlp/stanza/issues/1055#issuecomment-1744320976) errors as in the last sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esta es la primera frase.', 'Esta es la segunda frase.']\n",
      "['¿Cuántas veces, en un día típico, te detienes a reflexionar sobre la complejidad intrínseca de la existencia humana, con sus interconexiones profundas y su capacidad infinita para la sorpresa y la maravilla?', 'A medida que exploramos las vastas extensiones del conocimiento, ¿no te parece fascinante cómo se entrelazan las múltiples disciplinas científicas, desde la física cuántica hasta la biología molecular, revelando un universo de posibilidades aún por descubrir?']\n"
     ]
    }
   ],
   "source": [
    "# spanish\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize', verbose=False)\n",
    "\n",
    "two_sentences = 'Esta es la primera frase. Esta es la segunda frase.'\n",
    "doc = nlp(two_sentences)\n",
    "print([sent.text for sent in doc.sentences])\n",
    "\n",
    "two_complicated_sentences = (\n",
    "    '¿Cuántas veces, en un día típico, te detienes a reflexionar sobre la complejidad intrínseca de la existencia humana, con sus interconexiones profundas y su capacidad infinita para la sorpresa y la maravilla?'\n",
    "    ' '\n",
    "    'A medida que exploramos las vastas extensiones del conocimiento, ¿no te parece fascinante cómo se entrelazan las múltiples disciplinas científicas, desde la física cuántica hasta la biología molecular, revelando un universo de posibilidades aún por descubrir?'\n",
    "\n",
    ")\n",
    "doc = nlp(two_complicated_sentences)\n",
    "print([sent.text for sent in doc.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained tokenizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31414, 232, 6, 127, 766, 16, 24874, 1071, 328, 2]\n",
      "0\t<s>\n",
      "31414\tHello\n",
      "232\t world\n",
      "6\t,\n",
      "127\t my\n",
      "766\t name\n",
      "16\t is\n",
      "24874\t Hau\n",
      "1071\tke\n",
      "328\t!\n",
      "2\t</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>Hello world, my name is Hauke!</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# monolingual\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "encoding = tokenizer('Hello world, my name is Hauke!')\n",
    "print(encoding['input_ids'])\n",
    "for tok in encoding['input_ids']:\n",
    "    print(tok, tokenizer.decode(tok), sep='\\t')\n",
    "tokenizer.decode(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 35378, 8999, 4, 759, 9351, 83, 203755, 13, 38, 2], [0, 54029, 12389, 4, 8172, 15757, 443, 203755, 13, 38, 2]]\n",
      "0\t<s>\n",
      "35378\tHello\n",
      "8999\tworld\n",
      "4\t,\n",
      "759\tmy\n",
      "9351\tname\n",
      "83\tis\n",
      "203755\tHauk\n",
      "13\te\n",
      "38\t!\n",
      "2\t</s>\n",
      "\n",
      "0\t<s>\n",
      "54029\tHallo\n",
      "12389\tWelt\n",
      "4\t,\n",
      "8172\tmein\n",
      "15757\tName\n",
      "443\tist\n",
      "203755\tHauk\n",
      "13\te\n",
      "38\t!\n",
      "2\t</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multilingual\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "\n",
    "encoding = tokenizer(['Hello world, my name is Hauke!', 'Hallo Welt, mein Name ist Hauke!'])\n",
    "print(encoding['input_ids'])\n",
    "for sample in encoding['input_ids']:\n",
    "    for tok in sample:\n",
    "        print(tok, tokenizer.decode(tok), sep='\\t')\n",
    "    print(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: easyNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2M-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_model = EasyNMT('m2m_100_418M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Η', '▁εξε', 'ρεύ', 'νη', 'ση', '▁της', '▁ανθρώ', 'πι', 'νης', '▁φύ', 'σης', '▁είναι', '▁μια', '▁α', 'τέ', 'λει', 'ω', 'τη', '▁περι', 'πέ', 'τ', 'εια', '▁που', '▁μας', '▁καθ', 'ο', 'δη', 'γ', 'εί', '▁σε', '▁νέ', 'ες', '▁εν', 'νο', 'ιο', 'λογ', 'ικές', '▁δια', 'δρο', 'μές', '.']\n",
      "\n",
      "['▁Eesti', '▁rann', 'ikul', '▁jal', 'ut', 'ades', '▁võib', '▁tunnet', 'ada', '▁mere', '▁lõp', 'mat', 'ust', '▁ja', '▁aj', 'alo', 'ol', 'iste', '▁jut', 'ust', 'uste', '▁r', 'ikk', 'ust', '.']\n",
      "\n",
      "['▁Anadolu', \"'\", 'nun', '▁mis', 'tik', '▁atmosf', 'eri', ',', '▁tarih', '▁ve', '▁kültür', 'ün', '▁har', 'man', 'land', 'ığı', '▁bir', '▁yer', 'dir', '.']\n",
      "\n",
      "['▁Cie', 'ka', 'wo', 'ść', '▁i', '▁od', 'kry', 'wanie', '▁nowych', '▁koncep', 'cji', '▁stan', 'owią', '▁fundament', '▁naszej', '▁lud', 'zk', 'iej', '▁eg', 'zy', 'sten', 'cji', '.']\n",
      "\n",
      "['▁Рус', 'ская', '▁литера', 'тура', '▁откры', 'вает', '▁перед', '▁нами', '▁богат', 'ство', '▁челове', 'ческого', '▁опы', 'та', '▁и', '▁по', 'гру', 'жает', '▁нас', '▁в', '▁глу', 'би', 'ны', '▁челове', 'ческой', '▁души', '.']\n",
      "\n",
      "['▁Česká', '▁kraj', 'ina', '▁op', 'lý', 'vá', '▁mal', 'eb', 'nými', '▁zá', 'kout', 'í', 'mi', '▁a', '▁histor', 'ick', 'ými', '▁pam', 'át', 'kami', ',', '▁které', '▁vy', 'práv', 'ě', 'jí', '▁pří', 'bě', 'hy', '▁minul', 'osti', '.']\n",
      "\n",
      "['▁A', '▁divers', 'idade', '▁cultural', '▁do', '▁Brasil', '▁re', 'fl', 'ete', '-', 'se', '▁na', '▁fus', 'ão', '▁de', '▁influ', 'ências', '▁ind', 'í', 'gen', 'as', ',', '▁afric', 'anas', '▁e', '▁europ', 'ei', 'as', ',', '▁cri', 'ando', '▁uma', '▁rica', '▁tape', 'ç', 'aria', '▁de', '▁tradi', 'ções', '.']\n",
      "\n",
      "['▁De', '▁gra', 'chten', '▁van', '▁Amsterdam', '▁g', 'etu', 'igen', '▁van', '▁een', '▁rij', 'ke', '▁gesch', 'i', 'edenis', '▁en', '▁vor', 'men', '▁een', '▁intr', 'iger', 'end', '▁net', 'werk', '▁dat', '▁de', '▁stad', '▁do', 'ork', 'ru', 'ist', '.']\n",
      "\n",
      "['▁Norges', '▁maj', 'est', 'et', 'iske', '▁fj', 'order', '▁og', '▁bort', 'g', 'jem', 'te', '▁fj', 'ellt', 'op', 'per', '▁lok', 'ker', '▁event', 'yr', 'ly', 'st', 'ne', '▁reis', 'ende', '▁til', '▁å', '▁ut', 'for', 'ske', '▁natur', 'ens', '▁under', 'ver', 'ker', '.']\n",
      "\n",
      "['▁കേരള', 'ത്തിന്റെ', '▁സൗ', 'ന്ദ', 'ര്യം', '▁പ്ര', 'കട', 'മാ', 'ക്കുന്ന', '▁അ', 'ക്കാ', 'ദ', 'മ', 'ിക', '▁പരി', 'സ്ഥ', 'ി', 'തി', 'കൾ', '▁ഹി', 'മാ', 'ല', 'യ', 'ത്തിന്റെ', '▁സന്ദ', 'ർ', 'ശ', 'ന', 'ത്തിന്', '▁കേന്ദ്ര', 'ീക', 'രിക്കുന്നു', '.']\n",
      "\n",
      "['▁Sloven', 'ski', '▁jezik', '▁je', '▁bogat', '▁s', '▁svojimi', '▁idi', 'omat', 'skimi', '▁iz', 'razi', ',', '▁ki', '▁od', 'raž', 'ajo', '▁duhov', 'ito', '▁nara', 'vo', '▁in', '▁glob', 'oko', '▁č', 'ust', 'ven', 'ost', '.']\n",
      "\n",
      "['▁Del', 'ving', '▁into', '▁the', '▁intr', 'ica', 'cies', '▁of', '▁quant', 'um', '▁mechan', 'ics', '▁allows', '▁us', '▁to', '▁gr', 'asp', '▁the', '▁prof', 'o', 'und', '▁myst', 'eries', '▁that', '▁govern', '▁the', '▁fundamental', '▁asp', 'ects', '▁of', '▁the', '▁univers', 'e', '.']\n",
      "\n",
      "['▁Den', '▁danske', '▁hyg', 'ge', '▁kultur', '▁sk', 'aber', '▁en', '▁atmosf', 'ære', '▁af', '▁varme', '▁og', '▁samh', 'ør', 'ighed', ',', '▁der', '▁om', 'f', 'av', 'ner', '▁liv', 'ets', '▁enk', 'le', '▁glæ', 'der', '.']\n",
      "\n",
      "['▁Suomen', '▁lum', 'iset', '▁mais', 'emat', '▁tarjo', 'avat', '▁uno', 'ht', 'um', 'att', 'oman', '▁nä', 'k', 'ym', 'än', '▁poh', 'jo', 'isen', '▁luonn', 'on', '▁ka', 'une', 'udesta', '.']\n",
      "\n",
      "['▁Sverige', '▁är', '▁kän', 't', '▁för', '▁sin', '▁design', 'inn', 'ov', 'ation', '▁och', '▁det', '▁sätt', '▁på', '▁vilket', '▁den', '▁integr', 'eras', '▁i', '▁vard', 'agen', ',', '▁vilket', '▁sk', 'apar', '▁en', '▁harmon', 'isk', '▁liv', 'sst', 'il', '.']\n",
      "\n",
      "['▁La', '▁arquit', 'ectura', '▁gó', 'tica', '▁de', '▁la', '▁cat', 'edral', '▁de', '▁Barcelona', '▁es', '▁un', '▁testi', 'mon', 'io', '▁impresion', 'ante', '▁de', '▁la', '▁dest', 're', 'za', '▁art', 'ística', '▁y', '▁la', '▁devo', 'ción', '▁religi', 'osa', '.']\n",
      "\n",
      "['▁Die', '▁de', 'utsche', '▁Phil', 'os', 'ophie', '▁hat', '▁einen', '▁t', 'ief', 'gre', 'if', 'enden', '▁Ein', 'fl', 'uss', '▁auf', '▁das', '▁Den', 'ken', '▁und', '▁die', '▁intelle', 'ktu', 'elle', '▁Trad', 'ition', '▁welt', 'weit', '▁ausge', 'üb', 't', '.']\n",
      "\n",
      "['▁L', \"'\", 'arte', '▁cul', 'in', 'aria', '▁italiana', '▁è', '▁un', \"'\", 'esperienza', '▁sens', 'ori', 'ale', '▁che', '▁celebra', '▁l', \"'\", 'amore', '▁per', '▁gli', '▁ingredi', 'enti', '▁fres', 'chi', '▁e', '▁la', '▁conv', 'ivi', 'alità', '.']\n",
      "\n",
      "['▁L', \"'\", 'eff', 'erv', 'esc', 'ence', '▁cultur', 'elle', '▁de', '▁Paris', ',', '▁avec', '▁ses', '▁mus', 'ées', ',', '▁ses', '▁thé', 'â', 'tres', '▁et', '▁ses', '▁caf', 'és', ',', '▁fait', '▁de', '▁la', '▁ville', '▁une', '▁destination', '▁artisti', 'que', '▁incon', 'to', 'urn', 'able', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = mt_model.translator.tokenizer\n",
    "\n",
    "sents = [\n",
    "    'Η εξερεύνηση της ανθρώπινης φύσης είναι μια ατέλειωτη περιπέτεια που μας καθοδηγεί σε νέες εννοιολογικές διαδρομές.',\n",
    "    'Eesti rannikul jalutades võib tunnetada mere lõpmatust ja ajalooliste jutustuste rikkust.',\n",
    "    \"Anadolu'nun mistik atmosferi, tarih ve kültürün harmanlandığı bir yerdir.\",\n",
    "    'Ciekawość i odkrywanie nowych koncepcji stanowią fundament naszej ludzkiej egzystencji.',\n",
    "    'Русская литература открывает перед нами богатство человеческого опыта и погружает нас в глубины человеческой души.',\n",
    "    'Česká krajina oplývá malebnými zákoutími a historickými památkami, které vyprávějí příběhy minulosti.',\n",
    "    'A diversidade cultural do Brasil reflete-se na fusão de influências indígenas, africanas e europeias, criando uma rica tapeçaria de tradições.',\n",
    "    'De grachten van Amsterdam getuigen van een rijke geschiedenis en vormen een intrigerend netwerk dat de stad doorkruist.',\n",
    "    'Norges majestetiske fjorder og bortgjemte fjelltopper lokker eventyrlystne reisende til å utforske naturens underverker.',\n",
    "    'കേരളത്തിന്റെ സൗന്ദര്യം പ്രകടമാക്കുന്ന അക്കാദമിക പരിസ്ഥിതികൾ ഹിമാലയത്തിന്റെ സന്ദർശനത്തിന് കേന്ദ്രീകരിക്കുന്നു.',\n",
    "    'Slovenski jezik je bogat s svojimi idiomatskimi izrazi, ki odražajo duhovito naravo in globoko čustvenost.',\n",
    "    'Delving into the intricacies of quantum mechanics allows us to grasp the profound mysteries that govern the fundamental aspects of the universe.',\n",
    "    'Den danske hygge kultur skaber en atmosfære af varme og samhørighed, der omfavner livets enkle glæder.',\n",
    "    'Suomen lumiset maisemat tarjoavat unohtumattoman näkymän pohjoisen luonnon kauneudesta.',\n",
    "    'Sverige är känt för sin designinnovation och det sätt på vilket den integreras i vardagen, vilket skapar en harmonisk livsstil.',\n",
    "    'La arquitectura gótica de la catedral de Barcelona es un testimonio impresionante de la destreza artística y la devoción religiosa.',\n",
    "    'Die deutsche Philosophie hat einen tiefgreifenden Einfluss auf das Denken und die intellektuelle Tradition weltweit ausgeübt.',\n",
    "    \"L'arte culinaria italiana è un'esperienza sensoriale che celebra l'amore per gli ingredienti freschi e la convivialità.\",\n",
    "    \"L'effervescence culturelle de Paris, avec ses musées, ses théâtres et ses cafés, fait de la ville une destination artistique incontournable.\",\n",
    "]\n",
    "encodings = tokenizer(sents, truncation=False, padding=False, return_attention_mask=False, add_special_tokens=False)\n",
    "\n",
    "for tok_ids in encodings['input_ids']:\n",
    "    print([tokenizer.convert_ids_to_tokens(tok_id) for tok_id in tok_ids])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPUS-MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mt_model.translator.models['Helsinki-NLP/opus-mt-en-de']['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [16816, 2, 360, 68]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer('Hello, world!', return_attention_mask=False, add_special_tokens=False)\n",
    "print(encoding)\n",
    "tokenizer.decode(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Hello', ',', '▁world', '!']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Hello, world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b61304ba204149b33c07aa06f301de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e8fc5a41bb46b981c6f1e4742ec340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143cf6534d134053968fe18550f590ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e982a6b9164eada7323cb3edad6afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877c7aec399e4feb95b7a3dc4b3ad38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60d458b060645b28d24bc1a741a443d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b34c7c85f44bc6ba7580824cf315c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hallo Welt!'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_model.translate('Hello world!', source_lang='en', target_lang='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"\n",
    "English\ten\tThe suspense was unbearable … the door creaked open.\n",
    "English\ten\t━ remove all the stress from your life\n",
    "French\tfr\tIl a écrit un roman «Le Mystère» qui est devenu un best-seller.\n",
    "German\tde\tDie Spannung war unerträglich… die Tür quietschte.\n",
    "German\tde\tEr schrieb einen Roman „Das Geheimnis”, der ein Bestseller wurde.\n",
    "\"\"\"\n",
    "sents = [row.split('\\t') for row in  sents.split('\\n') if row != '']\n",
    "sents_df = pd.DataFrame(sents, columns=['language', 'code', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "--------------------\n",
      "Die Spannung war unerträglich… die Tür quietschte.\n",
      "Die Spannung war unerträglich... die Tür quietschte.\n",
      "\n",
      "Er schrieb einen Roman „Das Geheimnis”, der ein Bestseller wurde.\n",
      "Er schrieb einen Roman \"Das Geheimnis\", der ein Bestseller wurde.\n",
      "\n",
      "en\n",
      "--------------------\n",
      "The suspense was unbearable … the door creaked open.\n",
      "The suspense was unbearable ... the door creaked open.\n",
      "\n",
      "━ remove all the stress from your life\n",
      "- remove all the stress from your life\n",
      "\n",
      "fr\n",
      "--------------------\n",
      "Il a écrit un roman «Le Mystère» qui est devenu un best-seller.\n",
      "Il a écrit un roman \"Le Mystère\" qui est devenu un best-seller.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sacremoses import MosesPunctNormalizer # see https://github.com/hplt-project/sacremoses/blob/master/sacremoses/normalize.py\n",
    "for lang, d in sents_df.groupby('code'):\n",
    "    print(lang, '-'*20, sep='\\n')\n",
    "    punct_normalizer = MosesPunctNormalizer(lang=lang, pre_replace_unicode_punct=True, post_remove_control_chars=True)\n",
    "    for text in d.text:\n",
    "        print(text, punct_normalizer.normalize(text), '', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages with non-Latin scripts\n",
    "\n",
    "When we work with texts written in Spanish, English, German, French, and so on, we work with Romanic languages that use the Roman script and the Latin alphabet.\n",
    "However, there are many languages that use other scripts:\n",
    "\n",
    "- *Cyrillic script*: Used for Russian, Bulgarian, Serbian, and many other Slavic languages.\n",
    "- *Arabic script*: Used for Arabic, Persian, Urdu, and several other languages in the Middle East and North Africa.\n",
    "- *Devanagari script*: Used for Hindi, Sanskrit, Marathi, and other Indian languages.\n",
    "- *Hanzi*: Used for Mandarin, Cantonese, and other Chinese languages.\n",
    "- *Hangul script*: Used for Korean.\n",
    "- *Hebrew script*: Used for Hebrew.\n",
    "- *Greek script*: Used for Greek.\n",
    "- *Georgian script*: Used for Georgian.\n",
    "- *Katakana and Hiragana scripts*: Used for Japanese.\n",
    "- *Tamil script*: Used for Tamil.\n",
    "- *Thai script*: Used for Thai.\n",
    "- *Coptic script*: Used for the Coptic language.\n",
    "- *Armenian script*: Used for Armenian.\n",
    "- *Bengali script*: Used for Bengali and Assamese.\n",
    "- *Gurmukhi script*: Used for Punjabi.\n",
    "- *Khmer script*: Used for Khmer (Cambodian).\n",
    "- *Lao script*: Used for Lao.\n",
    "- *Mongolian script*: Used for Mongolian.\n",
    "- *Tibetan script*: Used for Tibetan.\n",
    "- *Syriac script*: Used for various languages in the Syriac community.\n",
    "\n",
    "Because of the historical developemnet of the field of computational linguistics, \n",
    "we have a wide range of tools and resources available pre-processing and tokenizing English texts and texts written in other Romanic languages, but fev tools and resources for other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translitaration\n",
    "\n",
    "Transliteration is the process of converting text or words from one script into another script.\n",
    "It means to represent the characters of one writing system with equivalent characters of another system.\n",
    "In linguistics, the purpose of transliteration is to facilitate the pronunciation or understanding of words written in a script that may be unfamiliar to the reader.\n",
    "But in machine translation and multilingual NLP, transliteration is also used as a common pre-processing step applied to non-Latin languages (e.g., [here](https://github.com/yannvgn/laserembeddings/blob/master/laserembeddings/preprocessing.py#L108))\n",
    "\n",
    "For example, when transliterating from the Cyrillic script to the Latin script, as in the case of Russian, the Cyrillic characters are replaced with their Latin counterparts while attempting to preserve the original pronunciation.\n",
    "Importantly, transliteration does *not* involve translation of the meaning of words; it is solely concerned with representing the characters of one script in another!\n",
    "\n",
    "### Transliteration systems\n",
    "\n",
    "To map characters from one script to another, we need a transliteration system that defines the rules for this process.\n",
    "There are different standards for transliterating between specific pairs of scripts.\n",
    "Common transliteration systems include the *International Alphabet of Sanskrit Transliteration* ([IAST](https://en.wikipedia.org/wiki/International_Alphabet_of_Sanskrit_Transliteration)) for Sanskrit and other Indic languages, and the [BGN/PCGN](https://en.wikipedia.org/wiki/BGN/PCGN_romanization) system for Romanization of Cyrillic scripts, among others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translitarating Cyrilic to Latin (\"Romanize\")\n",
    "\n",
    "There is a nice package for transliterating Cyrillic to Latin script in Python: [transliterate](https://github.com/barseghyanartur/transliterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"\n",
    "Armenian\thy\tԲարեւ աշխարհ!\tBarev ashkharh!\tHello, world!\n",
    "Bulgarian\tbg\tЗдравей, свят!\tZdravey, svyat!\tHello, world!\n",
    "Georgian\tka\tგამარჯობა, მსოფლიო!\tGamajoba, msoplio!\tHello, world!\n",
    "Greek\tel\tΓεια σου, κόσμε!\tYia sou, kosme!\tHello, world!\n",
    "Macedonian\tmk\tЗдраво, свету!\tZdravo, svetu!\tHello, world!\n",
    "Mongolian\tmn\tСайн байна уу, дэлхий!\tSain baina uu, delkhi!\tHello, world!\n",
    "Russian\tru\tПривет, мир!\tPrivet, mir!\tHello, world!\n",
    "Serbian\tsr\tЗдраво, свете!\tZdravo, svete!\tHello, world!\n",
    "Ukrainian\tuk\tПривіт, світ!\tPrivit, svit!\tHello, world!\n",
    "\"\"\"\n",
    "sents = [row.split('\\t') for row in  sents.split('\\n') if row != '']\n",
    "sents_df = pd.DataFrame(sents, columns=['language', 'code', 'text', 'transliteration', 'translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transliterate import translit, get_available_language_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(sents_df.code.isin(get_available_language_codes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hy\tԲարեւ աշխարհ!\tBareւ ashxarh!\n",
      "bg\tЗдравей, свят!\tZdravey, svyat!\n",
      "ka\tგამარჯობა, მსოფლიო!\tgamarjoba, msoflio!\n",
      "el\tΓεια σου, κόσμε!\tGeia soy, kosme!\n",
      "mk\tЗдраво, свету!\tZdravo, svetu!\n",
      "mn\tСайн байна уу, дэлхий!\tSain baina uu, delkhii!\n",
      "ru\tПривет, мир!\tPrivet, mir!\n",
      "sr\tЗдраво, свете!\tZdravo, svete!\n",
      "uk\tПривіт, світ!\tPryvit, svit!\n"
     ]
    }
   ],
   "source": [
    "for i, d in sents_df.iterrows():\n",
    "    print(d.code, d.text, translit(d.text, d.code, reversed=True), sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
